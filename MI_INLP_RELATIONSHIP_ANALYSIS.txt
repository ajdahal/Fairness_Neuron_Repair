====================================================================================================
MUTUAL INFORMATION vs INLP RESULTS: COMPREHENSIVE RELATIONSHIP ANALYSIS
====================================================================================================

====================================================================================================
PART 1: UNDERSTANDING STAGE1_BIAS_ANALYSIS.PY
====================================================================================================

SUMMARY OF THE INLP PIPELINE:
----------------------------------------------------------------------------------------------------

The stage1_bias_analysis.py script implements a complete INLP (Iterative Nullspace Projection) 
fairness intervention pipeline:

1. DATA PREPROCESSING:
   - Loads train/test data and applies HyperTransformer (one-hot encoding, normalization)
   - Extracts features and labels, converts to tensors
   - Maps gender labels (Female=0, Male=1) for SVM training

2. BASELINE MODEL TRAINING:
   - Trains a 3-layer or 4-layer neural network (input -> 128 -> 64 -> 1 or input -> 256 -> 128 -> 64 -> 1)
   - Uses BCEWithLogitsLoss with class weighting for imbalanced data
   - Trains for 200 epochs, tracks best validation loss, restores best model
   - Evaluates baseline accuracy, AOD, EOD, and counterfactual discrimination

3. ACTIVATION EXTRACTION:
   - Extracts activations from the last hidden layer (64 neurons) for all training/test samples
   - These activations represent the learned representations before the output layer

4. INLP PROCESS (Gender Information Removal):
   - Trains an SVM to detect gender from activations (bias detector)
   - Iteratively projects activations onto the nullspace of the gender direction
   - Each iteration: 
     a. Train SVM to find gender direction (weight vector w)
     b. Normalize to get unit vector u = w / ||w||
     c. Create projection matrix: P_step = I - α * u * u^T
     d. Project activations: X_new = X_old @ P_step^T
     e. Update cumulative projection: P = P @ P_step
   - Continues for n_iter iterations or until SVM accuracy drops to random (~50%)

5. WEIGHT SURGERY:
   - Applies the projection matrix P to the output layer weights: W_new = W_old @ P
   - Adjusts bias to account for data centering: b_new = b_old - W_new @ μ
   - This permanently modifies the model to remove gender information from predictions

6. FINAL EVALUATION:
   - Re-evaluates accuracy, AOD, EOD, counterfactual discrimination
   - Compares SVM accuracy before/after projection to verify gender removal

HOW GENDER INFORMATION IS REMOVED FROM ACTIVATIONS:
----------------------------------------------------------------------------------------------------

The INLP process removes gender information through geometric projection:

1. **Bias Direction Identification**: 
   - An SVM is trained on activations to predict gender
   - The SVM's weight vector w represents the "direction" in activation space that encodes gender
   - This direction is normalized to a unit vector u

2. **Nullspace Projection**:
   - Creates a projection matrix P_step = I - α * u * u^T
   - This matrix projects vectors onto the subspace orthogonal to the gender direction
   - α controls projection strength (0.8 = partial, 1.0 = full projection)

3. **Iterative Removal**:
   - After each projection, a new SVM is trained on the projected activations
   - If gender information remains, the process repeats
   - Each iteration removes one "dimension" of gender information
   - Multiple iterations can remove multiple correlated gender dimensions

4. **Weight Surgery**:
   - The cumulative projection matrix P is applied to output layer weights
   - This ensures that when the model processes new activations, they are automatically 
     projected to remove gender information
   - The projection is "baked into" the model weights, making it permanent

MATHEMATICAL INTUITION:
- If activations contain gender information, they have a component along direction u
- Projection removes this component: X_projected = X - (X · u) * u
- After projection, X_projected · u = 0 (orthogonal to gender direction)
- SVM can no longer predict gender from projected activations

====================================================================================================
PART 2: MUTUAL INFORMATION vs INLP RESULTS - COMPARATIVE ANALYSIS
====================================================================================================

DATASET-BY-DATASET ANALYSIS:
----------------------------------------------------------------------------------------------------

1. ADULT DATASET
----------------------------------------------------------------------------------------------------

Mutual Information Findings:
- Gender ↔ Income: MI = 0.0300 (WEAK direct relationship, Index 35)
- Gender ↔ Relationship: MI = 0.2700 (STRONG, Index 3)
- Gender ↔ Marital-status: MI = 0.1200 (MODERATE, Index 7)
- Gender ↔ Occupation: MI = 0.1000 (MODERATE, Index 12)
- Relationship ↔ Income: MI = 0.1200 (MODERATE, Index 8)
- Marital-status ↔ Income: MI = 0.1100 (MODERATE, Index 10)
- Highest MI: marital-status ↔ relationship (0.7300) - NOT involving gender

INLP Results (Best Configuration: iters=2, alpha=1.0):
- Accuracy Drop: 10.15%
- AOD Change: +10.16% (WORSE - contradiction!)
- EOD Change: -48.18% (IMPROVED significantly)
- SVM Before: 66.48%, SVM After: 44.74% (gender information reduced)

RELATIONSHIP ANALYSIS:
✓ MAKES SENSE:
  - Gender has WEAK direct relationship with income (MI=0.03), so removing gender should have 
    moderate impact on accuracy (10.15% drop is reasonable)
  - Gender has STRONG relationship with relationship/marital-status (proxy attributes)
  - These proxy attributes have MODERATE relationship with income
  - Removing gender information likely disrupts the proxy pathway: gender → relationship → income
  - This explains why accuracy drops: the model loses the ability to use relationship/marital-status 
    as gender proxies for income prediction
  - EOD improves significantly (-48%) because removing gender reduces conditional bias
  - AOD worsens (+10%) because average odds become less balanced (contradiction with EOD)

✗ CONTRADICTION:
  - AOD worsens while EOD improves - suggests the projection affects different outcome classes 
    differently, creating conditional bias

2. ACS_PUBLIC_COVERAGE DATASET
----------------------------------------------------------------------------------------------------

Mutual Information Findings:
- Gender ↔ Pubcov: MI = 0.0100 (VERY WEAK direct relationship, Index 68)
- Gender ↔ Fer (fertility): MI = 0.3100 (VERY STRONG, Index 2 - HIGHEST overall!)
- Gender ↔ Mar: MI = 0.0300 (WEAK, Index 33)
- Fer ↔ Pubcov: MI = 0.0000 (NO relationship, Index 102)
- Mar ↔ Pubcov: MI = 0.0100 (VERY WEAK, Index 81)

INLP Results (Best Configuration: iters=1, alpha=0.8):
- Accuracy Drop: 14.12%
- AOD Change: -14.17% (IMPROVED)
- EOD Change: +12.34% (WORSE - contradiction!)
- SVM Before: 68.24%, SVM After: 68.24% (with alpha=0.8, iters=1 - NO change!)

RELATIONSHIP ANALYSIS:
✓ MAKES SENSE:
  - Gender has VERY WEAK direct relationship with pubcov (MI=0.01), yet accuracy drops 14%
  - Gender has VERY STRONG relationship with fer (MI=0.31), but fer has NO relationship with pubcov
  - This suggests gender information is encoded in activations through fer, but fer itself doesn't 
    predict pubcov
  - Removing gender removes fer-related information, which may be correlated with other useful 
    features for pubcov prediction
  - The large accuracy drop (14%) despite weak direct MI suggests gender is a strong proxy for 
    other predictive features
  - AOD improves (-14%) because average bias is reduced
  - EOD worsens (+12%) - another contradiction suggesting conditional bias

✗ CONTRADICTION:
  - With alpha=0.8, iters=1, SVM accuracy unchanged (68.24%) - projection ineffective
  - Yet accuracy still drops 14% - suggests projection affects model even when gender detection 
    unchanged
  - EOD worsens while AOD improves - conditional bias issue

3. COMPAS DATASET
----------------------------------------------------------------------------------------------------

Mutual Information Findings:
- Gender ↔ Two_year_recid: MI = 0.0100 (VERY WEAK, Index 22)
- Gender ↔ Priors_count: MI = 0.0100 (VERY WEAK, Index 23)
- Gender ↔ Race: MI = 0.0000 (NO relationship, Index 24)
- Priors_count ↔ Two_year_recid: MI = 0.0500 (MODERATE, Index 2 - HIGHEST overall)
- Race ↔ Two_year_recid: MI = 0.0100 (WEAK, Index 17)

INLP Results (Best Configuration: iters=2, alpha=1.0):
- Accuracy Drop: 0.00% (NO DROP!)
- AOD Change: +2.49% (slightly worse)
- EOD Change: +4.22% (slightly worse)
- SVM Before: 59.27%, SVM After: 50.45% (gender information reduced to near-random)

RELATIONSHIP ANALYSIS:
✓ MAKES SENSE:
  - Gender has VERY WEAK relationship with recidivism (MI=0.01) and all other attributes
  - Gender is essentially INDEPENDENT of the prediction task
  - Removing gender information should have MINIMAL impact on accuracy (0% drop confirmed!)
  - The model doesn't rely on gender for predictions, so removing it doesn't hurt performance
  - Fairness metrics worsen slightly (+2.49% AOD, +4.22% EOD) - this is concerning but minimal
  - SVM drops to 50.45% (near-random) confirming gender removal, but this doesn't help fairness
  - This suggests gender wasn't a significant source of bias in the first place

4. CREDIT DATASET
----------------------------------------------------------------------------------------------------

Mutual Information Findings:
- Gender ↔ Class: MI = 0.0000 (NO relationship, Index 197)
- Gender ↔ Attribute18: MI = 0.0400 (WEAK, Index 25)
- Gender ↔ Attribute11: MI = 0.0400 (WEAK, Index 27)
- Gender ↔ Age: MI = 0.0300 (WEAK, Index 43)
- Attribute18 ↔ Class: MI = 0.0100 (VERY WEAK, Index 109)
- Attribute11 ↔ Class: MI = 0.0000 (NO relationship, Index 136)

INLP Results (All Configurations Identical):
- Accuracy Drop: -1.00% (actually IMPROVES!)
- AOD Change: +19.21% (WORSE significantly)
- EOD Change: 0.00% (NO change)
- SVM Before: 86.50%, SVM After: Varies (34.5% to 86.5% depending on config)

RELATIONSHIP ANALYSIS:
✗ ANOMALY - DOESN'T MAKE SENSE:
  - Gender has NO direct relationship with class (MI=0.00)
  - Gender-related attributes have WEAK/NO relationship with class
  - Yet AOD WORSENS by 19.21% after removing gender
  - Accuracy actually IMPROVES slightly (-1% = improvement)
  - All configurations produce IDENTICAL results - suggests projection not working or bug
  - This is the WORST fairness outcome despite gender being independent of the task
  - Possible explanations:
    a. Projection direction is incorrect (removing wrong information)
    b. Gender information in activations is correlated with useful features for prediction
    c. The projection creates new bias by disrupting feature relationships
    d. Dataset too small (800 samples) for robust projection

5. NLSB DATASET
----------------------------------------------------------------------------------------------------

Mutual Information Findings:
- Gender ↔ Income96gt17: MI = 0.0300 (WEAK, Index 18)
- Gender ↔ Height: MI = 0.3200 (VERY STRONG, Index 2 - HIGHEST overall!)
- Gender ↔ Weight: MI = 0.2200 (STRONG, Index 5)
- Gender ↔ Typejob90: MI = 0.0800 (MODERATE, Index 9)
- Height ↔ Income96gt17: MI = 0.0200 (VERY WEAK, Index 30)
- Weight ↔ Income96gt17: MI = 0.0200 (VERY WEAK, Index 29)
- Typejob90 ↔ Income96gt17: MI = 0.0300 (WEAK, Index 17)

INLP Results (Best Configuration: iters=1, alpha=0.8):
- Accuracy Drop: 0.71%
- AOD Change: -4.93% (IMPROVED)
- EOD Change: -2.65% (IMPROVED)
- SVM Before: 75.76%, SVM After: 75.76% (with alpha=0.8, iters=1 - NO change!)

RELATIONSHIP ANALYSIS:
✓ MAKES SENSE:
  - Gender has WEAK direct relationship with income (MI=0.03)
  - Gender has VERY STRONG relationship with physical attributes (height MI=0.32, weight MI=0.22)
  - But these physical attributes have WEAK relationship with income
  - This is a "decoupled proxy" scenario: gender → physical attributes → (weak link) → income
  - Removing gender removes physical attribute information, but since physical attributes don't 
    strongly predict income, accuracy impact is minimal (0.71% drop)
  - Both AOD and EOD improve slightly (-4.93%, -2.65%) - consistent improvement
  - SVM unchanged with conservative settings suggests gender information in activations is 
    primarily through physical attributes, which are already weakly predictive

6. BANK_MARKETING DATASET
----------------------------------------------------------------------------------------------------

Mutual Information Findings:
- Age (sensitive) ↔ Y: MI = 0.0100 (VERY WEAK, Index 86)
- Age ↔ Job: MI = 0.1100 (MODERATE, Index 9)
- Age ↔ Balance: MI = 0.0400 (WEAK, Index 29)
- Age ↔ Housing: MI = 0.0400 (WEAK, Index 32)
- Job ↔ Y: MI = 0.0200 (VERY WEAK, Index 54)
- Balance ↔ Y: MI = 0.0500 (WEAK, Index 24)
- Housing ↔ Y: MI = 0.0500 (WEAK, Index 25)
- Pdays ↔ Y: MI = 0.0800 (MODERATE, Index 13)

INLP Results:
- All log files incomplete/crashed - no data available
- Cannot analyze relationship

====================================================================================================
PART 3: PATTERN ANALYSIS BY SCENARIO TYPE
====================================================================================================

SCENARIO 1: GENDER STRONGLY RELATED TO OTHER ATTRIBUTES, WEAKLY TO OUTPUT LABEL
----------------------------------------------------------------------------------------------------
Examples: Adult, ACS_Public_Coverage, NLSB

Pattern:
- Gender has STRONG MI with proxy attributes (relationship, fer, height)
- Gender has WEAK MI with output label (income, pubcov, income96gt17)
- Proxy attributes have MODERATE/WEAK MI with output label

INLP Impact:
✓ Expected: Moderate to high accuracy drop (9-15%)
  - Removing gender disrupts proxy pathway: gender → proxy → output
  - Model loses ability to use proxy attributes effectively
  - Accuracy drops because proxy information becomes less useful

✓ Expected: Mixed fairness results
  - AOD may improve or worsen depending on how proxy attributes affect fairness
  - EOD often shows opposite trend from AOD (contradiction)
  - Fairness improvement is inconsistent

✓ Observed Results:
  - Adult: 10.15% acc drop, AOD +10% (worse), EOD -48% (better) - CONTRADICTION
  - ACS: 14.12% acc drop, AOD -14% (better), EOD +12% (worse) - CONTRADICTION
  - NLSB: 0.71% acc drop, AOD -4.93% (better), EOD -2.65% (better) - CONSISTENT

Interpretation:
- When gender is strongly encoded through proxies, removing gender disrupts the proxy pathway
- This causes accuracy loss but fairness impact is unpredictable
- The contradiction between AOD and EOD suggests the projection affects different outcome classes 
  differently, creating conditional bias

SCENARIO 2: GENDER STRONGLY RELATED TO OUTPUT LABEL
----------------------------------------------------------------------------------------------------
Examples: None found in our datasets (all show weak gender-label MI)

Hypothetical Pattern:
- Gender has STRONG MI with output label (e.g., MI > 0.1)
- Gender has MODERATE/STRONG MI with other attributes

INLP Impact (Hypothetical):
✓ Expected: High accuracy drop (15-25%+)
  - Removing gender removes direct predictive information
  - Model loses significant predictive power
  - Accuracy should drop substantially

✓ Expected: Strong fairness improvement
  - Removing direct gender-label relationship should improve fairness
  - AOD and EOD should both improve significantly
  - But accuracy cost would be prohibitive

Real-World Note:
- None of our datasets show this pattern (all have MI < 0.03 between gender and label)
- This suggests real-world datasets rarely have direct gender-label relationships
- Bias is typically indirect through proxy attributes

SCENARIO 3: GENDER WEAKLY RELATED TO OTHER ATTRIBUTES, WEAKLY TO OUTPUT LABEL
----------------------------------------------------------------------------------------------------
Examples: Compas, Credit

Pattern:
- Gender has WEAK/NO MI with most attributes
- Gender has WEAK/NO MI with output label
- Gender is essentially independent of the prediction task

INLP Impact:
✓ Expected: Minimal accuracy drop (0-2%)
  - Gender information is not used for prediction
  - Removing it should have minimal impact
  - Model doesn't rely on gender

✓ Expected: Minimal fairness change
  - If gender isn't a source of bias, removing it shouldn't change fairness much
  - Fairness metrics should remain stable

✓ Observed Results:
  - Compas: 0% acc drop, AOD +2.49% (slightly worse), EOD +4.22% (slightly worse)
  - Credit: -1% acc drop (improves!), AOD +19.21% (significantly worse) - ANOMALY

Interpretation:
- Compas: Results make sense - minimal impact as expected
- Credit: ANOMALY - fairness worsens despite gender independence
  * Possible causes: projection creates new bias, incorrect projection direction, or dataset 
    characteristics (very small, early convergence)

SCENARIO 4: GENDER WEAKLY RELATED TO OTHER ATTRIBUTES, STRONGLY TO OUTPUT LABEL
----------------------------------------------------------------------------------------------------
Examples: None found in our datasets

Hypothetical Pattern:
- Gender has WEAK MI with other attributes
- Gender has STRONG MI with output label (direct relationship)

INLP Impact (Hypothetical):
✓ Expected: Very high accuracy drop (20-30%+)
  - Removing direct gender-label relationship removes core predictive information
  - Model would lose significant accuracy

✓ Expected: Strong fairness improvement
  - Removing direct bias should improve fairness significantly
  - But accuracy cost would be prohibitive for practical use

Real-World Note:
- This scenario is rare - direct gender-label relationships are uncommon
- When they exist, they're often legitimate (e.g., gender-specific medical conditions)
- Removing them may not be desirable from a fairness perspective

====================================================================================================
PART 4: KEY INSIGHTS AND CONTRADICTIONS
====================================================================================================

1. THE PROXY ATTRIBUTE PARADOX:
----------------------------------------------------------------------------------------------------
When gender has strong MI with proxy attributes (relationship, fer, height) that have moderate MI 
with the output label:

- Removing gender disrupts the proxy pathway
- Accuracy drops significantly (9-15%) even though gender-label MI is weak
- Fairness results are inconsistent (AOD and EOD often contradict)

This suggests:
- Models learn to use gender through proxy attributes
- Removing gender breaks these learned associations
- The disruption affects accuracy more than direct gender-label MI would suggest

2. THE INDEPENDENCE ANOMALY (Credit Dataset):
----------------------------------------------------------------------------------------------------
When gender is independent of the task (MI=0.00 with label):

- Expected: Minimal impact on accuracy and fairness
- Observed: Fairness WORSE (AOD +19.21%) despite independence
- This contradicts expectations

Possible explanations:
- Projection removes information correlated with useful features
- Small dataset (800 samples) leads to unstable projections
- Projection direction may be incorrect
- Gender information in activations is entangled with task-relevant features despite MI=0

3. THE AOD-EOD CONTRADICTION:
----------------------------------------------------------------------------------------------------
Multiple datasets show AOD and EOD moving in opposite directions:

- Adult: AOD +10% (worse), EOD -48% (better)
- ACS: AOD -14% (better), EOD +12% (worse)

This suggests:
- INLP affects different outcome classes differently
- Average odds (AOD) and equalized odds (EOD) measure different aspects of fairness
- Removing gender may reduce bias for one class but increase it for another
- The projection creates conditional bias (bias that depends on the outcome class)

4. THE SVM-ACCURACY DISCONNECT:
----------------------------------------------------------------------------------------------------
Some configurations show SVM accuracy unchanged but accuracy still drops:

- ACS (alpha=0.8, iters=1): SVM unchanged (68.24%), but accuracy drops 14%
- NLSB (alpha=0.8, iters=1): SVM unchanged (75.76%), but accuracy drops 0.71%

This suggests:
- Projection affects model predictions even when gender detection unchanged
- The projection may remove information correlated with gender but not directly gender
- Model may use gender-correlated features that aren't detected by SVM
- Weight surgery changes model behavior beyond just gender removal

5. THE PHYSICAL ATTRIBUTE DECOUPLING (NLSB):
----------------------------------------------------------------------------------------------------
NLSB shows gender strongly related to physical attributes (height, weight) but these have weak 
relationship with income:

- Gender → Height (MI=0.32), Height → Income (MI=0.02)
- This creates a "decoupled proxy" - strong gender-proxy link, weak proxy-label link
- Result: Minimal accuracy impact (0.71%) when removing gender
- Both AOD and EOD improve consistently

This is the IDEAL scenario for INLP:
- Gender information is primarily through attributes that don't strongly predict the label
- Removing gender has minimal accuracy cost
- Fairness improves consistently

====================================================================================================
PART 5: RECOMMENDATIONS BASED ON MUTUAL INFORMATION PATTERNS
====================================================================================================

1. BEFORE APPLYING INLP, CHECK MUTUAL INFORMATION:
----------------------------------------------------------------------------------------------------
- Calculate MI between gender and output label
- Calculate MI between gender and all other attributes
- Calculate MI between gender-related attributes and output label

2. PREDICT INLP IMPACT BASED ON MI PATTERNS:
----------------------------------------------------------------------------------------------------
- Strong gender-proxy MI + Moderate proxy-label MI → Expect 10-15% accuracy drop
- Weak gender-label MI + Weak gender-attribute MI → Expect 0-2% accuracy drop
- Strong gender-label MI → Expect 20%+ accuracy drop (may not be worth it)

3. DATASET-SPECIFIC GUIDELINES:
----------------------------------------------------------------------------------------------------
- Adult: Use conservative settings (iters=1-2, alpha=0.8-1.0) due to proxy pathway disruption
- ACS: Expect high accuracy cost (14%+) due to fer relationship, use iters=1
- Compas: Safe to use (0% accuracy impact), but fairness improvement minimal
- Credit: AVOID - anomaly suggests projection harmful despite gender independence
- NLSB: IDEAL scenario - use conservative settings for best results

4. MONITOR CONTRADICTIONS:
----------------------------------------------------------------------------------------------------
- Watch for AOD-EOD contradictions - indicates conditional bias
- If AOD and EOD move in opposite directions, the projection may be creating new bias
- Consider which fairness metric is more important for your use case

5. VERIFY GENDER REMOVAL:
----------------------------------------------------------------------------------------------------
- Check SVM accuracy after projection - should drop to ~50% (random)
- If SVM unchanged but accuracy drops, investigate proxy attribute relationships
- If SVM drops but fairness doesn't improve, gender may not be the primary bias source

====================================================================================================
CONCLUSION
====================================================================================================

The relationship between mutual information and INLP results reveals important patterns:

1. **Direct gender-label MI is weak in all datasets** (MI < 0.03), yet accuracy drops are 
   significant (9-15%) in some cases. This suggests models use gender through proxy attributes.

2. **Proxy attribute pathways** (gender → proxy → label) explain accuracy drops better than direct 
   gender-label relationships. When gender is strongly related to proxies that predict the label, 
   removing gender disrupts this pathway.

3. **Fairness improvements are inconsistent** and often contradictory (AOD vs EOD). This suggests 
   INLP affects different outcome classes differently, creating conditional bias.

4. **The credit dataset anomaly** shows that even when gender is independent (MI=0), removing it can 
   worsen fairness. This highlights the complexity of bias in learned representations.

5. **The NLSB dataset** represents the ideal scenario: gender strongly related to attributes that 
   don't strongly predict the label. This allows gender removal with minimal accuracy cost and 
   consistent fairness improvement.

KEY TAKEAWAY:
Mutual information provides valuable insights for predicting INLP impact, but the relationship is 
complex. Proxy attribute pathways, conditional bias, and representation-level entanglement create 
challenges that simple MI values cannot fully capture. Careful analysis of MI patterns combined with 
empirical testing is essential for successful fairness intervention.

====================================================================================================

